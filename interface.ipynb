{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "\n",
    "# Predefined prompts\n",
    "prompts = [\n",
    "    \"Tell me a joke about programming\",\n",
    "    \"Write a short story about a time-traveling robot\",\n",
    "    \"Explain quantum computing to a 5-year-old\",\n",
    "    \"Create a recipe for the most unusual pizza\",\n",
    "    \"Describe an alien civilization's first contact with Earth\"\n",
    "]\n",
    "\n",
    "def respond(\n",
    "    message,\n",
    "    history: list[tuple[str, str]],\n",
    "    system_message,\n",
    "    max_tokens,\n",
    "    temperature,\n",
    "    top_p,\n",
    "):\n",
    "    if not message:\n",
    "        return []\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "    for val in history:\n",
    "        if val[0]:\n",
    "            messages.append({\"role\": \"user\", \"content\": val[0]})\n",
    "        if val[1]:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": val[1]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    prompt = \"\\n\".join([m[\"content\"] for m in messages])\n",
    "    response = \"\"\n",
    "\n",
    "    try:\n",
    "        for chunk in client.text_generation(\n",
    "            prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            stream=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        ):\n",
    "            if isinstance(chunk, str):\n",
    "                response += chunk\n",
    "            else:\n",
    "                response += chunk.token.text if hasattr(chunk, 'token') else chunk.generated_text\n",
    "            yield [(message, response)]\n",
    "    except Exception as e:\n",
    "        yield [(message, f\"An error occurred: {str(e)}\")]\n",
    "\n",
    "def update_textbox(prompt):\n",
    "    return gr.update(value=prompt)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(label=\"Type your message or select a prompt\")\n",
    "    with gr.Row():\n",
    "        prompt_dropdown = gr.Dropdown(choices=[\"\"] + prompts, label=\"Select a premade prompt\", value=\"\")\n",
    "        submit = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    with gr.Accordion(\"Advanced options\", open=False):\n",
    "        system = gr.Textbox(value=\"You are a friendly Chatbot.\", label=\"System message\")\n",
    "        max_tokens = gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\")\n",
    "        temperature = gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\")\n",
    "        top_p = gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p (nucleus sampling)\")\n",
    "\n",
    "    prompt_dropdown.change(update_textbox, inputs=[prompt_dropdown], outputs=[msg])\n",
    "\n",
    "    submit.click(respond, [msg, chatbot, system, max_tokens, temperature, top_p], chatbot)\n",
    "    msg.submit(respond, [msg, chatbot, system, max_tokens, temperature, top_p], chatbot)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
